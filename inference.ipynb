{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence to Sequence ChatBot Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference on seq2seq chatbot, Using CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from jieba import cut\n",
    "from p3self.lprint import lprint\n",
    "from multiprocessing import Pool\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from constants import *\n",
    "CUDA = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class s2s_data(Dataset):\n",
    "    def __init__(self,load_io, vocab_in, vocab_out, seq_addr, build_seq=False,\n",
    "                 build_vocab = False,):\n",
    "        \"\"\"\n",
    "        vocab_in,vocab_out are csv file addresses\n",
    "        \"\"\"\n",
    "        self.load_io=load_io\n",
    "        self.vocab_in = vocab_in\n",
    "        self.vocab_out = vocab_out\n",
    "        self.seq_addr = seq_addr\n",
    "        \n",
    "        print(\"[Loading the sequence data]\")\n",
    "        \n",
    "        if build_seq:\n",
    "            self.i,self.o = self.load_io()\n",
    "            np.save(self.seq_addr,[self.i,self.o])\n",
    "        else:\n",
    "            [self.i,self.o] = np.load(self.seq_addr).tolist()\n",
    "        print(\"[Sequence data loaded]\")\n",
    "            \n",
    "        assert len(self.i)==len(self.o),\"input seq length mush match output seq length\"\n",
    "        \n",
    "        self.N = len(self.i)\n",
    "        print(\"Length of sequence:\\t\",self.N)\n",
    "        \n",
    "        if build_vocab:\n",
    "            self.vocab_i = self.build_vocab(self.i)\n",
    "            self.vocab_o = self.build_vocab(self.o)\n",
    "            \n",
    "            self.vocab_i.to_csv(self.vocab_in)\n",
    "            self.vocab_o.to_csv(self.vocab_out)\n",
    "            \n",
    "            self.print_vocab_info()\n",
    "        else:\n",
    "            self.vocab_i = pd.read_csv(self.vocab_in).fillna(\"\")\n",
    "            self.vocab_o = pd.read_csv(self.vocab_out).fillna(\"\")\n",
    "                  \n",
    "            self.print_vocab_info()\n",
    "        \n",
    "        print(\"building mapping dicts\")\n",
    "        self.i_char2idx,self.i_idx2char = self.get_mapping(self.vocab_i)\n",
    "        self.o_char2idx,self.o_idx2char = self.get_mapping(self.vocab_o)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.N\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        return self.seq2idx(self.i[idx],self.mapfunc_i),self.seq2idx(self.o[idx],self.mapfunc_o)\n",
    "    \n",
    "    def get_full_token(self,list_of_tokens):\n",
    "        \"\"\"\n",
    "        From a list of list of tokens, to a long list of tokens, duplicate tokens included\n",
    "        \"\"\"\n",
    "        return (\" \".join(list_of_tokens)).split(\" \")\n",
    "    \n",
    "    def get_mapping(self,vocab_df):\n",
    "        char2idx=dict(zip(vocab_df[\"token\"],vocab_df[\"idx\"]))\n",
    "        idx2char=dict(zip(vocab_df[\"idx\"],vocab_df[\"token\"]))\n",
    "        return char2idx,idx2char\n",
    "    \n",
    "    def seq2idx(self,x,mapfunc):\n",
    "        return np.vectorize(mapfunc)(x.split(\" \")).tolist()\n",
    "    \n",
    "    def mapfunc_i(self,x):\n",
    "        try:\n",
    "            return self.i_char2idx[x]\n",
    "        except:\n",
    "            return 2\n",
    "        \n",
    "    def mapfunc_o(self,x):\n",
    "        try:\n",
    "            return self.o_char2idx[x]\n",
    "        except:\n",
    "            return 2\n",
    "        \n",
    "    def get_token_count_dict(self,full_token):\n",
    "        \"\"\"count the token to a list\"\"\"\n",
    "        return Counter(full_token)\n",
    "    \n",
    "    def build_vocab(self,seq_list):\n",
    "        ct_dict = self.get_token_count_dict(self.get_full_token(seq_list))\n",
    "        ct_dict[\"SOS_TOKEN\"] = 9e9\n",
    "        ct_dict[\"EOS_TOKEN\"] = 8e9\n",
    "        ct_dict[\" \"] = 7e9\n",
    "        tk,ct = list(ct_dict.keys()),list(ct_dict.values())\n",
    "        \n",
    "        token_df=pd.DataFrame({\"token\":tk,\"count\":ct}).sort_values(by=\"count\",ascending=False)\n",
    "        return token_df.reset_index().drop(\"index\",axis=1).reset_index().rename(columns={\"index\":\"idx\"}).fillna(\"\")\n",
    "    \n",
    "    def print_vocab_info(self):\n",
    "        self.vocab_size_i = len(self.vocab_i)\n",
    "        self.vocab_size_o = len(self.vocab_o)\n",
    "        \n",
    "        print(\"[in seq vocab address]: %s,\\t%s total lines\"%(self.vocab_in,self.vocab_size_i))\n",
    "        print(\"[out seq vocab address]: %s,\\t%s total lines\"%(self.vocab_out,self.vocab_size_o))\n",
    "            \n",
    "        print(\"Input sequence vocab samples:\")\n",
    "        print(self.vocab_i.sample(5))\n",
    "        print(\"Output sequence vocab samples:\")\n",
    "        print(self.vocab_o.sample(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_empty():\n",
    "    return list(range(5)),list(range(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class inf_s2s(s2s_data):\n",
    "    def __init__(self,vocab_in, vocab_out):\n",
    "        super(inf_s2s,self).__init__(load_empty, vocab_in, vocab_out, seq_addr=\"/data/chat/empty.npy\", build_seq=True,\n",
    "                 build_vocab = False,)\n",
    "        \n",
    "    def feed_encoder(self,x):\n",
    "        if CN_SEG:\n",
    "            x_list = list(cut(x))\n",
    "        else:\n",
    "            x_list = list(str(x))\n",
    "        arr = np.array(self.seq2idx(\" \".join(x_list),self.mapfunc_o))\n",
    "        return torch.LongTensor(arr).unsqueeze(0)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Loading the sequence data]\n",
      "[Sequence data loaded]\n",
      "Length of sequence:\t 5\n",
      "[in seq vocab address]: /data/dict/chat_vocab_in.csv,\t5748 total lines\n",
      "[out seq vocab address]: /data/dict/chat_vocab_out.csv,\t5635 total lines\n",
      "Input sequence vocab samples:\n",
      "      Unnamed: 0   idx  count token\n",
      "3232        3232  3232    9.0     捧\n",
      "2156        2156  2156   42.0     胞\n",
      "947          947   947  283.0     顺\n",
      "3022        3022  3022   12.0     仨\n",
      "3776        3776  3776    5.0     挚\n",
      "Output sequence vocab samples:\n",
      "      Unnamed: 0   idx   count token\n",
      "673          673   673   880.0     嘴\n",
      "2273        2273  2273    68.0     噎\n",
      "3273        3273  3273    15.0     讶\n",
      "569          569   569  1085.0     7\n",
      "5258        5258  5258     1.0     怆\n",
      "building mapping dicts\n"
     ]
    }
   ],
   "source": [
    "inf=inf_s2s(vocab_in = DICT_IN,\n",
    "         vocab_out = DICT_OUT,)\n",
    "# inf=inf_s2s(vocab_in = DICT_IN,\n",
    "#          vocab_out = \"/data/dict/chat_vocab_char_out.csv\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  67,  185,  795,  280,  332,    6]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inf.feed_encoder(\"很高兴认识你\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import EncoderRNN_GRU as EncoderRNN\n",
    "from models import DecoderRNN_GRU as DecoderRNN\n",
    "\n",
    "encoder = EncoderRNN(inf.vocab_size_i,HIDDEN_SIZE,n_layers = NB_LAYER)\n",
    "decoder = DecoderRNN(HIDDEN_SIZE,inf.vocab_size_o,n_layers = NB_LAYER)\n",
    "\n",
    "encoder.cuda_ = False\n",
    "decoder.cuda_ = False\n",
    "\n",
    "def load_s2s(version):\n",
    "    encoder.load_state_dict(torch.load(\"/data/weights/enc_%s.pkl\"%(version)))\n",
    "    decoder.load_state_dict(torch.load(\"/data/weights/dec_%s.pkl\"%(version)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the following encounter error (it's because the trainning process is saving), try run it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_s2s(VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer(question):\n",
    "    encoder_hidden = encoder.initHidden(1)\n",
    "    last_idx= decoder.initInput(1)\n",
    "    encoder_output,encoder_hidden = encoder(question,encoder_hidden)\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "    \n",
    "    i = 0\n",
    "    output=list()\n",
    "    while i < MAX_LEN:\n",
    "        if (i>0 and last_idx.item() == SOS_TOKEN):\n",
    "            break\n",
    "        decoder_input,decoder_hidden = decoder(last_idx,decoder_hidden)\n",
    "        last_idx = torch.max(decoder_input,dim=-1)[1]\n",
    "        output.append(last_idx.item())\n",
    "        i += 1\n",
    "    output_char = \" \".join(np.vectorize(lambda x:inf.o_idx2char[x])(output).tolist())\n",
    "    print(output_char)\n",
    "    print(\"length:\\t\",len(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "那 有 呢 呢 到 律 法 规 了 了 ， 还 不 然 了 不 到 的 着 打\n",
      "length:\t 20\n"
     ]
    }
   ],
   "source": [
    "answer(inf.feed_encoder(\"为什么事情会这样\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "哎 呦 呦 哎 … … … ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~\n",
      "length:\t 20\n"
     ]
    }
   ],
   "source": [
    "answer(inf.feed_encoder(\"你叫什么名字\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=   = SOS_TOKEN\n",
      "length:\t 5\n"
     ]
    }
   ],
   "source": [
    "answer(inf.feed_encoder(\"你喜欢不喜欢我\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "有 个 人 醒 呢 ， 主 人 醒 爱 你 的 那 个 快 ~ ~ ~ ~ ~\n",
      "length:\t 20\n"
     ]
    }
   ],
   "source": [
    "answer(inf.feed_encoder(\"你把我灌醉\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n 你 么 么 SOS_TOKEN\n",
      "length:\t 5\n"
     ]
    }
   ],
   "source": [
    "answer(inf.feed_encoder(\"等你等到我心碎\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=   = SOS_TOKEN\n",
      "length:\t 5\n"
     ]
    }
   ],
   "source": [
    "answer(inf.feed_encoder(\"唱首歌给我听\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你 有 呢 呢 呢 d 呢 纸 为 SOS_TOKEN\n",
      "length:\t 10\n"
     ]
    }
   ],
   "source": [
    "answer(inf.feed_encoder(\"你是男的还是女的\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "哎 呦 不 要 哎 哎 啊 切 糕 糕 糕 糕 糕 糕 糕 厉 厉 厉 厉 ！\n",
      "length:\t 20\n"
     ]
    }
   ],
   "source": [
    "answer(inf.feed_encoder(\"去你大爷的\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
