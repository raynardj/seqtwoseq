{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence to Sequence ChatBot Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference on seq2seq chatbot, Using CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from jieba import cut\n",
    "from p3self.lprint import lprint\n",
    "from multiprocessing import Pool\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BS = 512# Batch size\n",
    "\n",
    "VOCAB_SEQ_IN = 3000\n",
    "VOCAB_SEQ_OUT = 3000\n",
    "\n",
    "SOS_TOKEN = 0\n",
    "EOS_TOKEN = 1\n",
    "\n",
    "LR = 5e-3\n",
    "HIDDEN_SIZE = 512\n",
    "MAX_LEN = 20\n",
    "\n",
    "VERSION = \"0.0.2\"\n",
    "# \"0.0.1\" chars\n",
    "# \"0.0.2\" token\n",
    "\n",
    "CUDA = False\n",
    "\n",
    "CN_SEG = True\n",
    "\n",
    "if CN_SEG:\n",
    "    DICT_IN = \"/data/dict/chat_vocab_in.csv\"\n",
    "    DICT_OUT = \"/data/dict/chat_vocab_out.csv\"\n",
    "    SEQ_DIR = \"/data/chat/xhj_seq.npy\"\n",
    "else:\n",
    "    DICT_IN = \"/data/dict/chat_vocab_in.csv\"\n",
    "    DICT_OUT = \"/data/dict/chat_vocab_out.csv\"\n",
    "    SEQ_DIR = \"/data/chat/xhj_seq_char.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class s2s_data(Dataset):\n",
    "    def __init__(self,load_io, vocab_in, vocab_out, seq_addr, build_seq=False,\n",
    "                 build_vocab = False,):\n",
    "        \"\"\"\n",
    "        vocab_in,vocab_out are csv file addresses\n",
    "        \"\"\"\n",
    "        self.load_io=load_io\n",
    "        self.vocab_in = vocab_in\n",
    "        self.vocab_out = vocab_out\n",
    "        self.seq_addr = seq_addr\n",
    "        \n",
    "        print(\"[Loading the sequence data]\")\n",
    "        \n",
    "        if build_seq:\n",
    "            self.i,self.o = self.load_io()\n",
    "            np.save(self.seq_addr,[self.i,self.o])\n",
    "        else:\n",
    "            [self.i,self.o] = np.load(self.seq_addr).tolist()\n",
    "        print(\"[Sequence data loaded]\")\n",
    "            \n",
    "        assert len(self.i)==len(self.o),\"input seq length mush match output seq length\"\n",
    "        \n",
    "        self.N = len(self.i)\n",
    "        print(\"Length of sequence:\\t\",self.N)\n",
    "        \n",
    "        if build_vocab:\n",
    "            self.vocab_i = self.build_vocab(self.i)\n",
    "            self.vocab_o = self.build_vocab(self.o)\n",
    "            \n",
    "            self.vocab_i.to_csv(self.vocab_in)\n",
    "            self.vocab_o.to_csv(self.vocab_out)\n",
    "            \n",
    "            self.print_vocab_info()\n",
    "        else:\n",
    "            self.vocab_i = pd.read_csv(self.vocab_in).fillna(\"\")\n",
    "            self.vocab_o = pd.read_csv(self.vocab_out).fillna(\"\")\n",
    "                  \n",
    "            self.print_vocab_info()\n",
    "        \n",
    "        print(\"building mapping dicts\")\n",
    "        self.i_char2idx,self.i_idx2char = self.get_mapping(self.vocab_i)\n",
    "        self.o_char2idx,self.o_idx2char = self.get_mapping(self.vocab_o)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.N\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        return self.seq2idx(self.i[idx],self.mapfunc_i),self.seq2idx(self.o[idx],self.mapfunc_o)\n",
    "    \n",
    "    def get_full_token(self,list_of_tokens):\n",
    "        \"\"\"\n",
    "        From a list of list of tokens, to a long list of tokens, duplicate tokens included\n",
    "        \"\"\"\n",
    "        return (\" \".join(list_of_tokens)).split(\" \")\n",
    "    \n",
    "    def get_mapping(self,vocab_df):\n",
    "        char2idx=dict(zip(vocab_df[\"token\"],vocab_df[\"idx\"]))\n",
    "        idx2char=dict(zip(vocab_df[\"idx\"],vocab_df[\"token\"]))\n",
    "        return char2idx,idx2char\n",
    "    \n",
    "    def seq2idx(self,x,mapfunc):\n",
    "        return np.vectorize(mapfunc)(x.split(\" \")).tolist()\n",
    "    \n",
    "    def mapfunc_i(self,x):\n",
    "        try:\n",
    "            return self.i_char2idx[x]\n",
    "        except:\n",
    "            return 2\n",
    "        \n",
    "    def mapfunc_o(self,x):\n",
    "        try:\n",
    "            return self.o_char2idx[x]\n",
    "        except:\n",
    "            return 2\n",
    "        \n",
    "    def get_token_count_dict(self,full_token):\n",
    "        \"\"\"count the token to a list\"\"\"\n",
    "        return Counter(full_token)\n",
    "    \n",
    "    def build_vocab(self,seq_list):\n",
    "        ct_dict = self.get_token_count_dict(self.get_full_token(seq_list))\n",
    "        ct_dict[\"SOS_TOKEN\"] = 9e9\n",
    "        ct_dict[\"EOS_TOKEN\"] = 8e9\n",
    "        ct_dict[\" \"] = 7e9\n",
    "        tk,ct = list(ct_dict.keys()),list(ct_dict.values())\n",
    "        \n",
    "        token_df=pd.DataFrame({\"token\":tk,\"count\":ct}).sort_values(by=\"count\",ascending=False)\n",
    "        return token_df.reset_index().drop(\"index\",axis=1).reset_index().rename(columns={\"index\":\"idx\"}).fillna(\"\")\n",
    "    \n",
    "    def print_vocab_info(self):\n",
    "        self.vocab_size_i = len(self.vocab_i)\n",
    "        self.vocab_size_o = len(self.vocab_o)\n",
    "        \n",
    "        print(\"[in seq vocab address]: %s,\\t%s total lines\"%(self.vocab_in,self.vocab_size_i))\n",
    "        print(\"[out seq vocab address]: %s,\\t%s total lines\"%(self.vocab_out,self.vocab_size_o))\n",
    "            \n",
    "        print(\"Input sequence vocab samples:\")\n",
    "        print(self.vocab_i.sample(5))\n",
    "        print(\"Output sequence vocab samples:\")\n",
    "        print(self.vocab_o.sample(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_empty():\n",
    "    return list(range(5)),list(range(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class inf_s2s(s2s_data):\n",
    "    def __init__(self,vocab_in, vocab_out):\n",
    "        super(inf_s2s,self).__init__(load_empty, vocab_in, vocab_out, seq_addr=\"/data/chat/empty.npy\", build_seq=True,\n",
    "                 build_vocab = False,)\n",
    "        \n",
    "    def feed_encoder(self,x):\n",
    "        if CN_SEG:\n",
    "            x_list = list(cut(x))\n",
    "        else:\n",
    "            x_list = list(str(x))\n",
    "        arr = np.array(self.seq2idx(\" \".join(x_list),self.mapfunc_o))\n",
    "        return torch.LongTensor(arr).unsqueeze(0)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Loading the sequence data]\n",
      "[Sequence data loaded]\n",
      "Length of sequence:\t 5\n",
      "[in seq vocab address]: /data/dict/chat_vocab_in.csv,\t62596 total lines\n",
      "[out seq vocab address]: /data/dict/chat_vocab_out.csv,\t55508 total lines\n",
      "Input sequence vocab samples:\n",
      "       Unnamed: 0    idx  count token\n",
      "18424       18424  18424    3.0   张无忌\n",
      "32355       32355  32355    2.0    奶有\n",
      "61283       61283  61283    1.0   时志诚\n",
      "21932       21932  21932    3.0    拍个\n",
      "55102       55102  55102    1.0   人生观\n",
      "Output sequence vocab samples:\n",
      "       Unnamed: 0    idx  count token\n",
      "31898       31898  31898    2.0   父皇母\n",
      "17166       17166  17166    6.0   凑合着\n",
      "42525       42525  42525    1.0   十五号\n",
      "35713       35713  35713    2.0  静下心来\n",
      "4319         4319   4319   43.0    洋洋\n",
      "building mapping dicts\n"
     ]
    }
   ],
   "source": [
    "inf=inf_s2s(vocab_in = DICT_IN,\n",
    "         vocab_out = DICT_OUT,)\n",
    "# inf=inf_s2s(vocab_in = DICT_IN,\n",
    "#          vocab_out = \"/data/dict/chat_vocab_char_out.csv\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Loading model cost 0.891 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[  52,  684,  211,    7]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inf.feed_encoder(\"很高兴认识你\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_layers=1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True, num_layers=n_layers)\n",
    "        \n",
    "    def forward(self, input_, hidden):\n",
    "        output, hidden = self.gru(self.embedding(input_), hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    # TODO: other inits\n",
    "    def initHidden(self, batch_size):\n",
    "        en_hidden = torch.zeros(1, batch_size, self.hidden_size)\n",
    "        if CUDA:\n",
    "            en_hidden = en_hidden.cuda()\n",
    "        return en_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, n_layers=1):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True, num_layers=n_layers)\n",
    "        # TODO use transpose of embedding\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.sm = nn.LogSoftmax(dim=-1)\n",
    "        \n",
    "    def forward(self, input_, hidden):\n",
    "        emb = self.embedding(input_).unsqueeze(1)\n",
    "        # NB: Removed relu\n",
    "        res, hidden = self.gru(emb, hidden)\n",
    "        output = self.sm(self.out(res[:,0]))\n",
    "        return output, hidden\n",
    "    \n",
    "    def initInput(self,batch_size):\n",
    "        decoder_input = torch.LongTensor([SOS_TOKEN]*batch_size)\n",
    "        if CUDA:\n",
    "            decoder_input = decoder_input.cuda()\n",
    "        return decoder_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = EncoderRNN(inf.vocab_size_i,HIDDEN_SIZE)\n",
    "decoder = DecoderRNN(HIDDEN_SIZE,inf.vocab_size_o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_s2s(version):\n",
    "    encoder.load_state_dict(torch.load(\"/data/weights/enc_%s.pkl\"%(version)))\n",
    "    decoder.load_state_dict(torch.load(\"/data/weights/dec_%s.pkl\"%(version)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the following encounter error (it's because the trainning process is saving), try run it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_s2s(VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer(question):\n",
    "    encoder_hidden = encoder.initHidden(1)\n",
    "    last_idx= decoder.initInput(1)\n",
    "    print(encoder_hidden.size(),last_idx.size())\n",
    "    encoder_output,encoder_hidden = encoder(question,encoder_hidden)\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "    \n",
    "    i = 0\n",
    "    output=list()\n",
    "    while i < MAX_LEN:\n",
    "        if (i>0 and last_idx.item() == SOS_TOKEN):\n",
    "            break\n",
    "        decoder_input,decoder_hidden = decoder(last_idx,decoder_hidden)\n",
    "        last_idx = torch.max(decoder_input,dim=-1)[1]\n",
    "        output.append(last_idx.item())\n",
    "        i += 1\n",
    "    output_char = \" \".join(np.vectorize(lambda x:inf.o_idx2char[x])(output).tolist())\n",
    "    print(output_char)\n",
    "    print(\"length:\\t\",len(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 512]) torch.Size([1])\n",
      "SOS_TOKEN\n",
      "length:\t 1\n"
     ]
    }
   ],
   "source": [
    "answer(inf.feed_encoder(\"很高兴认识你，哈哈哈哈\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
