{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence to Sequence\n",
    "\n",
    "## Chat Bot Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import & Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using Theano backend.\n",
      "WARNING (theano.configdefaults): install mkl with `conda install mkl-service`: No module named 'mkl'\n",
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import RMSprop,Adam\n",
    "from jieba import cut\n",
    "from p3self.lprint import lprint\n",
    "from p3self.matchbox import Trainer\n",
    "from multiprocessing import Pool\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BS = 16# Batch size\n",
    "\n",
    "VOCAB_SEQ_IN = 3000\n",
    "VOCAB_SEQ_OUT = 3000\n",
    "\n",
    "SOS_TOKEN = 0\n",
    "EOS_TOKEN = 1\n",
    "\n",
    "LR = 5e-3\n",
    "HIDDEN_SIZE = 512\n",
    "\n",
    "VERSION = \"0.0.2\"\n",
    "# \"0.0.1\" chars\n",
    "# \"0.0.2\" token\n",
    "\n",
    "CUDA = torch.cuda.is_available()\n",
    "\n",
    "CN_SEG = True\n",
    "\n",
    "if CN_SEG:\n",
    "    DICT_IN = \"/data/dict/chat_vocab_in.csv\"\n",
    "    DICT_OUT = \"/data/dict/chat_vocab_out.csv\"\n",
    "    SEQ_DIR = \"/data/chat/xhj_seq.npy\"\n",
    "else:\n",
    "    DICT_IN = \"/data/dict/chat_vocab_in.csv\"\n",
    "    DICT_OUT = \"/data/dict/chat_vocab_out.csv\"\n",
    "    SEQ_DIR = \"/data/chat/xhj_seq_char.npy\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_hj_line(x):\n",
    "    return tuple(list(i[2:] for i in x.split(\"\\n\")))\n",
    "\n",
    "def cut_tkless(x):\n",
    "    return \" \".join(list(str(x)))\n",
    "\n",
    "def cutline(x):\n",
    "    return \" \".join(list(cut(x)))\n",
    "\n",
    "def load_xiaowangji():\n",
    "    file = open(\"/data/chat/Dialog_Corpus/xiaohuangji50w_nofenci.conv\")\n",
    "    f=file.read()[2:]\n",
    "    conv_block = f.split(\"\\nE\\n\")\n",
    "    conv_block\n",
    "    \n",
    "    p=Pool(6)\n",
    "    conv_list=p.map(read_hj_line,conv_block)\n",
    "    q,a=zip(*conv_list)\n",
    "    \n",
    "    q_l = p.map(cutline,q)\n",
    "    a_l = p.map(cutline,a)\n",
    "    \n",
    "    file.close()\n",
    "    return q_l,a_l\n",
    "\n",
    "def load_xwj_tk_less():\n",
    "    file = open(\"/data/chat/Dialog_Corpus/xiaohuangji50w_nofenci.conv\")\n",
    "    f=file.read()[2:]\n",
    "    conv_block = f.split(\"\\nE\\n\")\n",
    "    conv_block\n",
    "    \n",
    "    p=Pool(6)\n",
    "    conv_list=p.map(read_hj_line,conv_block)\n",
    "    q,a=zip(*conv_list)\n",
    "    \n",
    "    q_l = p.map(cut_tkless,q)\n",
    "    a_l = p.map(cut_tkless,a)\n",
    "    \n",
    "    file.close()\n",
    "    return q_l,a_l\n",
    "\n",
    "if CN_SEG:\n",
    "    LOAD_FUNC = load_xiaowangji\n",
    "else:\n",
    "    LOAD_FUNC = load_xwj_tk_less"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class s2s_data(Dataset):\n",
    "    def __init__(self,load_io, vocab_in, vocab_out, seq_addr, build_seq=False,\n",
    "                 build_vocab = False,):\n",
    "        \"\"\"\n",
    "        vocab_in,vocab_out are csv file addresses\n",
    "        \"\"\"\n",
    "        self.load_io=load_io\n",
    "        self.vocab_in = vocab_in\n",
    "        self.vocab_out = vocab_out\n",
    "        self.seq_addr = seq_addr\n",
    "        \n",
    "        print(\"[Loading the sequence data]\")\n",
    "        \n",
    "        if build_seq:\n",
    "            self.i,self.o = self.load_io()\n",
    "            np.save(self.seq_addr,[self.i,self.o])\n",
    "        else:\n",
    "            [self.i,self.o] = np.load(self.seq_addr).tolist()\n",
    "        print(\"[Sequence data loaded]\")\n",
    "            \n",
    "        assert len(self.i)==len(self.o),\"input seq length mush match output seq length\"\n",
    "        \n",
    "        self.N = len(self.i)\n",
    "        print(\"Length of sequence:\\t\",self.N)\n",
    "        \n",
    "        if build_vocab:\n",
    "            self.vocab_i = self.build_vocab(self.i)\n",
    "            self.vocab_o = self.build_vocab(self.o)\n",
    "            \n",
    "            self.vocab_i.to_csv(self.vocab_in)\n",
    "            self.vocab_o.to_csv(self.vocab_out)\n",
    "            \n",
    "            self.print_vocab_info()\n",
    "        else:\n",
    "            self.vocab_i = pd.read_csv(self.vocab_in).fillna(\"\")\n",
    "            self.vocab_o = pd.read_csv(self.vocab_out).fillna(\"\")\n",
    "                  \n",
    "            self.print_vocab_info()\n",
    "        \n",
    "        print(\"building mapping dicts\")\n",
    "        self.i_char2idx,self.i_idx2char = self.get_mapping(self.vocab_i)\n",
    "        self.o_char2idx,self.o_idx2char = self.get_mapping(self.vocab_o)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.N\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        return self.seq2idx(self.i[idx],self.mapfunc_i),self.seq2idx(self.o[idx],self.mapfunc_o)\n",
    "    \n",
    "    def get_full_token(self,list_of_tokens):\n",
    "        \"\"\"\n",
    "        From a list of list of tokens, to a long list of tokens, duplicate tokens included\n",
    "        \"\"\"\n",
    "        return (\" \".join(list_of_tokens)).split(\" \")\n",
    "    \n",
    "    def get_mapping(self,vocab_df):\n",
    "        char2idx=dict(zip(vocab_df[\"token\"],vocab_df[\"idx\"]))\n",
    "        idx2char=dict(zip(vocab_df[\"idx\"],vocab_df[\"token\"]))\n",
    "        return char2idx,idx2char\n",
    "    \n",
    "    def seq2idx(self,x,mapfunc):\n",
    "        return np.vectorize(mapfunc)(x.split(\" \")).tolist()\n",
    "    \n",
    "    def mapfunc_i(self,x):\n",
    "        try:\n",
    "            return self.i_char2idx[x]\n",
    "        except:\n",
    "            return 2\n",
    "        \n",
    "    def mapfunc_o(self,x):\n",
    "        try:\n",
    "            return self.o_char2idx[x]\n",
    "        except:\n",
    "            return 2\n",
    "        \n",
    "    def get_token_count_dict(self,full_token):\n",
    "        \"\"\"count the token to a list\"\"\"\n",
    "        return Counter(full_token)\n",
    "    \n",
    "    def build_vocab(self,seq_list):\n",
    "        ct_dict = self.get_token_count_dict(self.get_full_token(seq_list))\n",
    "        ct_dict[\"SOS_TOKEN\"] = 9e9\n",
    "        ct_dict[\"EOS_TOKEN\"] = 8e9\n",
    "        ct_dict[\" \"] = 7e9\n",
    "        tk,ct = list(ct_dict.keys()),list(ct_dict.values())\n",
    "        \n",
    "        token_df=pd.DataFrame({\"token\":tk,\"count\":ct}).sort_values(by=\"count\",ascending=False)\n",
    "        return token_df.reset_index().drop(\"index\",axis=1).reset_index().rename(columns={\"index\":\"idx\"}).fillna(\"\")\n",
    "    \n",
    "    def print_vocab_info(self):\n",
    "        self.vocab_size_i = len(self.vocab_i)\n",
    "        self.vocab_size_o = len(self.vocab_o)\n",
    "        \n",
    "        print(\"[in seq vocab address]: %s,\\t%s total lines\"%(self.vocab_in,self.vocab_size_i))\n",
    "        print(\"[out seq vocab address]: %s,\\t%s total lines\"%(self.vocab_out,self.vocab_size_o))\n",
    "            \n",
    "        print(\"Input sequence vocab samples:\")\n",
    "        print(self.vocab_i.sample(5))\n",
    "        print(\"Output sequence vocab samples:\")\n",
    "        print(self.vocab_o.sample(5))\n",
    "\n",
    "# We have to self difine a collate function\n",
    "# becuz we take the longest sequence lengnth with in a batch as the seq length for the entire batch\n",
    "def pad_collate(batch):\n",
    "    i,o = zip(*batch)\n",
    "    i_arr = pad_sequences(i,padding=\"post\",)\n",
    "    o_arr = pad_sequences(o,padding=\"post\",)\n",
    "    return torch.LongTensor(i_arr), torch.LongTensor(o_arr)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Loading the sequence data]\n",
      "[Sequence data loaded]\n",
      "Length of sequence:\t 454131\n",
      "[in seq vocab address]: /data/dict/chat_vocab_in.csv,\t62596 total lines\n",
      "[out seq vocab address]: /data/dict/chat_vocab_out.csv,\t55508 total lines\n",
      "Input sequence vocab samples:\n",
      "       Unnamed: 0    idx  count token\n",
      "44965       44965  44965    1.0   周煜超\n",
      "38483       38483  38483    1.0    喝得\n",
      "23393       23393  23393    2.0    摇头\n",
      "4700         4700   4700   22.0    养鸡\n",
      "25131       25131  25131    2.0     愚\n",
      "Output sequence vocab samples:\n",
      "       Unnamed: 0    idx  count token\n",
      "21823       21823  21823    4.0    平头\n",
      "40837       40837  40837    1.0    瓜业\n",
      "3800         3800   3800   52.0  早睡早起\n",
      "27682       27682  27682    3.0    傻类\n",
      "26246       26246  26246    3.0    du\n",
      "building mapping dicts\n"
     ]
    }
   ],
   "source": [
    "ds = s2s_data(LOAD_FUNC,\n",
    "                         DICT_IN,\n",
    "                         DICT_OUT,\n",
    "                         SEQ_DIR,\n",
    "                         build_seq=False,\n",
    "                         build_vocab=False)\n",
    "dl = DataLoader(ds,\n",
    "                batch_size=BS,collate_fn=pad_collate)\n",
    "\n",
    "dl_gen = iter(dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seq2Seq Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_layers=1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True, num_layers=n_layers)\n",
    "        \n",
    "    def forward(self, input_, hidden):\n",
    "        output, hidden = self.gru(self.embedding(input_), hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    # TODO: other inits\n",
    "    def initHidden(self, batch_size):\n",
    "        en_hidden = torch.zeros(1, batch_size, self.hidden_size)\n",
    "        if CUDA:\n",
    "            en_hidden = en_hidden.cuda()\n",
    "        return en_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, n_layers=1):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True, num_layers=n_layers)\n",
    "        # TODO use transpose of embedding\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.sm = nn.LogSoftmax()\n",
    "        \n",
    "    def forward(self, input_, hidden):\n",
    "        emb = self.embedding(input_).unsqueeze(1)\n",
    "        # NB: Removed relu\n",
    "        res, hidden = self.gru(emb, hidden)\n",
    "        output = self.sm(self.out(res[:,0]))\n",
    "        return output, hidden\n",
    "    \n",
    "    def initInput(self,batch_size):\n",
    "        decoder_input = torch.LongTensor([SOS_TOKEN]*batch_size)\n",
    "        if CUDA:\n",
    "            decoder_input = decoder_input.cuda()\n",
    "        return decoder_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = EncoderRNN(dl.dataset.vocab_size_i,HIDDEN_SIZE)\n",
    "decoder = DecoderRNN(HIDDEN_SIZE,dl.dataset.vocab_size_o)\n",
    "criterion = nn.NLLLoss()\n",
    "if CUDA:\n",
    "    encoder.cuda()\n",
    "    decoder.cuda()\n",
    "    criterion.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EncoderRNN(\n",
      "  (embedding): Embedding(62596, 512)\n",
      "  (gru): GRU(512, 512, batch_first=True)\n",
      ")\n",
      "DecoderRNN(\n",
      "  (embedding): Embedding(55508, 512)\n",
      "  (gru): GRU(512, 512, batch_first=True)\n",
      "  (out): Linear(in_features=512, out_features=55508, bias=True)\n",
      "  (sm): LogSoftmax()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(encoder)\n",
    "print(decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_opt = RMSprop(encoder.parameters(), lr=LR)\n",
    "de_opt = RMSprop(decoder.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_s2s():\n",
    "    torch.save(encoder.state_dict(), \"/data/weights/enc_%s.pkl\"%(VERSION))\n",
    "    torch.save(decoder.state_dict(), \"/data/weights/dec_%s.pkl\"%(VERSION))\n",
    "    \n",
    "def load_s2s(version):\n",
    "    encoder.load_state_dict(torch.load(\"/data/weights/enc_%s.pkl\"%(version)))\n",
    "    decoder.load_state_dict(torch.load(\"/data/weights/dec_%s.pkl\"%(version)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_s2s(VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_action(*args,**kwargs):\n",
    "    s1,s2 = args[0]\n",
    "    ite = kwargs[\"ite\"]\n",
    "    if CUDA:\n",
    "        s1,s2 = s1.cuda(),s2.cuda()\n",
    "        \n",
    "    batch_size = s1.size()[0]\n",
    "    target_length = s2.size()[1]\n",
    "    \n",
    "    en_opt.zero_grad()\n",
    "    de_opt.zero_grad()\n",
    "    \n",
    "    encoder_hidden = encoder.initHidden(batch_size)\n",
    "    encoder_output, encoder_hidden = encoder(s1,encoder_hidden)\n",
    "    \n",
    "    decoder_hidden = encoder_hidden # encoder passing hidden state to decoder!\n",
    "    \n",
    "    decoder_input  = decoder.initInput(batch_size)\n",
    "    \n",
    "    loss = 0\n",
    "    for seq_idx in range(target_length):\n",
    "        decoder_output, decoder_hidden = decoder(decoder_input,decoder_hidden)\n",
    "        \n",
    "        idx_target = s2[:,seq_idx]\n",
    "        \n",
    "        loss += criterion(decoder_output,idx_target)\n",
    "        decoder_input = idx_target # teacher forcing\n",
    "        \n",
    "    loss.backward()\n",
    "    \n",
    "    en_opt.step()\n",
    "    de_opt.step()\n",
    "    \n",
    "    if ite%5==4:\n",
    "        save_s2s()\n",
    "    \n",
    "    return {\n",
    "        \"loss\":loss.item(),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(dataset=ds,batch_size=BS,print_on=2)\n",
    "trainer.train_data.collate_fn = pad_collate\n",
    "trainer.action = train_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/28384 [00:00<?, ?it/s]/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/ipykernel_launcher.py:14: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  \n",
      "⭐[ep_0_i_11707]\tloss\t49.258:  41%|████      | 11708/28384 [1:43:36<2:27:34,  1.88it/s] "
     ]
    }
   ],
   "source": [
    "trainer.train(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
