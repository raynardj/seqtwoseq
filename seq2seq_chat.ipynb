{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence to Sequence\n",
    "\n",
    "## Chat Bot Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import & Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import RMSprop,Adam\n",
    "from jieba import cut\n",
    "from p3self.lprint import lprint\n",
    "from p3self.matchbox import Trainer\n",
    "from multiprocessing import Pool\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BS = 5# Batch size\n",
    "\n",
    "VOCAB_SEQ_IN = 3000\n",
    "VOCAB_SEQ_OUT = 3000\n",
    "\n",
    "SOS_TOKEN = 0\n",
    "EOS_TOKEN = 1\n",
    "\n",
    "LR = 5e-3\n",
    "HIDDEN_SIZE = 256\n",
    "\n",
    "CUDA = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_hj_line(x):\n",
    "    return tuple(list(i[2:] for i in x.split(\"\\n\")))\n",
    "\n",
    "def cut_tkless(x):\n",
    "    return \" \".join(list(str(x)))\n",
    "\n",
    "def cutline(x):\n",
    "    return \" \".join(list(cut(x)))\n",
    "\n",
    "def load_xiaowangji():\n",
    "    file = open(\"/data/chat/Dialog_Corpus/xiaohuangji50w_nofenci.conv\")\n",
    "    f=file.read()[2:]\n",
    "    conv_block = f.split(\"\\nE\\n\")\n",
    "    conv_block\n",
    "    \n",
    "    p=Pool(6)\n",
    "    conv_list=p.map(read_hj_line,conv_block)\n",
    "    q,a=zip(*conv_list)\n",
    "    \n",
    "    q_l = p.map(cutline,q)\n",
    "    a_l = p.map(cutline,a)\n",
    "    \n",
    "    file.close()\n",
    "    return q_l,a_l\n",
    "\n",
    "def load_xwj_tk_less():\n",
    "    file = open(\"/data/chat/Dialog_Corpus/xiaohuangji50w_nofenci.conv\")\n",
    "    f=file.read()[2:]\n",
    "    conv_block = f.split(\"\\nE\\n\")\n",
    "    conv_block\n",
    "    \n",
    "    p=Pool(6)\n",
    "    conv_list=p.map(read_hj_line,conv_block)\n",
    "    q,a=zip(*conv_list)\n",
    "    \n",
    "    q_l = p.map(cut_tkless,q)\n",
    "    a_l = p.map(cut_tkless,a)\n",
    "    \n",
    "    file.close()\n",
    "    return q_l,a_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class s2s_data(Dataset):\n",
    "    def __init__(self,load_io, vocab_in, vocab_out, seq_addr, build_seq=False,\n",
    "                 build_vocab = False,):\n",
    "        \"\"\"\n",
    "        vocab_in,vocab_out are csv file addresses\n",
    "        \"\"\"\n",
    "        self.load_io=load_io\n",
    "        self.vocab_in = vocab_in\n",
    "        self.vocab_out = vocab_out\n",
    "        self.seq_addr = seq_addr\n",
    "        \n",
    "        print(\"[Loading the sequence data]\")\n",
    "        \n",
    "        if build_seq:\n",
    "            self.i,self.o = self.load_io()\n",
    "            np.save(self.seq_addr,[self.i,self.o])\n",
    "        else:\n",
    "            [self.i,self.o] = np.load(self.seq_addr).tolist()\n",
    "        print(\"[Sequence data loaded]\")\n",
    "            \n",
    "        assert len(self.i)==len(self.o),\"input seq length mush match output seq length\"\n",
    "        \n",
    "        self.N = len(self.i)\n",
    "        print(\"Length of sequence:\\t\",self.N)\n",
    "        \n",
    "        if build_vocab:\n",
    "            self.vocab_i = self.build_vocab(self.i)\n",
    "            self.vocab_o = self.build_vocab(self.o)\n",
    "            \n",
    "            self.vocab_i.to_csv(self.vocab_in)\n",
    "            self.vocab_o.to_csv(self.vocab_out)\n",
    "            \n",
    "            self.print_vocab_info()\n",
    "        else:\n",
    "            self.vocab_i = pd.read_csv(self.vocab_in).fillna(\"\")\n",
    "            self.vocab_o = pd.read_csv(self.vocab_out).fillna(\"\")\n",
    "                  \n",
    "            self.print_vocab_info()\n",
    "        \n",
    "        print(\"building mapping dicts\")\n",
    "        self.i_char2idx,self.i_idx2char = self.get_mapping(self.vocab_i)\n",
    "        self.o_char2idx,self.o_idx2char = self.get_mapping(self.vocab_o)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.N\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        return self.seq2idx(self.i[idx],self.i_char2idx),self.seq2idx(self.o[idx],self.o_char2idx)\n",
    "    \n",
    "    def get_full_token(self,list_of_tokens):\n",
    "        \"\"\"\n",
    "        From a list of list of tokens, to a long list of tokens, duplicate tokens included\n",
    "        \"\"\"\n",
    "        return (\" \".join(list_of_tokens)).split(\" \")\n",
    "    \n",
    "    def get_mapping(self,vocab_df):\n",
    "        char2idx=dict(zip(vocab_df[\"token\"],vocab_df[\"idx\"]))\n",
    "        idx2char=dict(zip(vocab_df[\"idx\"],vocab_df[\"token\"]))\n",
    "        return char2idx,idx2char\n",
    "    \n",
    "    def seq2idx(self,x,mapdict):\n",
    "        return np.vectorize(lambda i:mapdict[i])(x.split(\" \")).tolist()\n",
    "    \n",
    "    def get_token_count_dict(self,full_token):\n",
    "        \"\"\"count the token to a list\"\"\"\n",
    "        return Counter(full_token)\n",
    "    \n",
    "    def build_vocab(self,seq_list):\n",
    "        ct_dict = self.get_token_count_dict(self.get_full_token(seq_list))\n",
    "        ct_dict[\"SOS_TOKEN\"] = 9e9\n",
    "        ct_dict[\"EOS_TOKEN\"] = 8e9\n",
    "        tk,ct = list(ct_dict.keys()),list(ct_dict.values())\n",
    "        \n",
    "        token_df=pd.DataFrame({\"token\":tk,\"count\":ct}).sort_values(by=\"count\",ascending=False)\n",
    "        return token_df.reset_index().drop(\"index\",axis=1).reset_index().rename(columns={\"index\":\"idx\"}).fillna(\"\")\n",
    "    \n",
    "    def print_vocab_info(self):\n",
    "        self.vocab_size_i = len(self.vocab_i)\n",
    "        self.vocab_size_o = len(self.vocab_o)\n",
    "        \n",
    "        print(\"[in seq vocab address]: %s,\\t%s total lines\"%(self.vocab_in,self.vocab_size_i))\n",
    "        print(\"[out seq vocab address]: %s,\\t%s total lines\"%(self.vocab_out,self.vocab_size_o))\n",
    "            \n",
    "        print(\"Input sequence vocab samples:\")\n",
    "        print(self.vocab_i.sample(5))\n",
    "        print(\"Output sequence vocab samples:\")\n",
    "        print(self.vocab_o.sample(5))\n",
    "\n",
    "# We have to self difine a collate function\n",
    "# becuz we take the longest sequence lengnth with in a batch as the seq length for the entire batch\n",
    "def pad_collate(batch):\n",
    "    i,o = zip(*batch)\n",
    "    i_arr = pad_sequences(i,padding=\"post\",)\n",
    "    o_arr = pad_sequences(o,padding=\"post\",)\n",
    "    return torch.LongTensor(i_arr), torch.LongTensor(o_arr)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dl = DataLoader(s2s_data(load_xiaowangji,\n",
    "#                          \"/data/dict/chat_vocab_in.csv\",\n",
    "#                          \"/data/dict/chat_vocab_out.csv\",\n",
    "#                          \"/data/chat/xhj_seq.npy\",\n",
    "#                          build_seq=False,\n",
    "#                          build_vocab=False),\n",
    "#                 batch_size=BS)\n",
    "\n",
    "# dl_gen = iter(dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Loading the sequence data]\n",
      "[Sequence data loaded]\n",
      "Length of sequence:\t 454131\n",
      "[in seq vocab address]: /data/dict/chat_vocab_char_in.csv,\t5747 total lines\n",
      "[out seq vocab address]: /data/dict/chat_vocab_char_out.csv,\t5634 total lines\n",
      "Input sequence vocab samples:\n",
      "      Unnamed: 0   idx  count token\n",
      "2581        2581  2581   23.0     浆\n",
      "793          793   793  377.0     功\n",
      "475          475   475  844.0     眼\n",
      "4861        4861  4861    2.0     龍\n",
      "5495        5495  5495    1.0     谀\n",
      "Output sequence vocab samples:\n",
      "      Unnamed: 0   idx    count token\n",
      "1415        1415  1415    253.0     兰\n",
      "4749        4749  4749      2.0     거\n",
      "19            19    19  31077.0     有\n",
      "5022        5022  5022      2.0     跹\n",
      "406          406   406   1696.0     服\n",
      "building mapping dicts\n"
     ]
    }
   ],
   "source": [
    "ds = s2s_data(load_xwj_tk_less,\n",
    "                         \"/data/dict/chat_vocab_char_in.csv\",\n",
    "                         \"/data/dict/chat_vocab_char_out.csv\",\n",
    "                         \"/data/chat/xhj_seq_char.npy\",\n",
    "                         build_seq = False,\n",
    "                         build_vocab = False)\n",
    "# dl = DataLoader(ds,\n",
    "#                 batch_size=BS,\n",
    "#                 collate_fn=pad_collate)\n",
    "\n",
    "# dl_gen = iter(dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seq2Seq Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_layers=1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True, num_layers=n_layers)\n",
    "        \n",
    "    def forward(self, input_, hidden):\n",
    "        output, hidden = self.gru(self.embedding(input_), hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    # TODO: other inits\n",
    "    def initHidden(self, batch_size):\n",
    "        en_hidden = torch.zeros(1, batch_size, self.hidden_size)\n",
    "        if CUDA:\n",
    "            en_hidden = en_hidden.cuda()\n",
    "        return en_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, n_layers=1):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True, num_layers=n_layers)\n",
    "        # TODO use transpose of embedding\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.sm = nn.LogSoftmax()\n",
    "        \n",
    "    def forward(self, input_, hidden):\n",
    "        emb = self.embedding(input_).unsqueeze(1)\n",
    "        # NB: Removed relu\n",
    "        res, hidden = self.gru(emb, hidden)\n",
    "        output = self.sm(self.out(res[:,0]))\n",
    "        return output, hidden\n",
    "    \n",
    "    def initInput(self,batch_size):\n",
    "        decoder_input = torch.LongTensor([SOS_TOKEN]*batch_size)\n",
    "        if CUDA:\n",
    "            decoder_input = decoder_input.cuda()\n",
    "        return decoder_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder = EncoderRNN(dl.dataset.vocab_size_i,HIDDEN_SIZE)\n",
    "decoder = DecoderRNN(HIDDEN_SIZE,dl.dataset.vocab_size_o)\n",
    "criterion = nn.NLLLoss()\n",
    "if CUDA:\n",
    "    encoder.cuda()\n",
    "    decoder.cuda()\n",
    "    criterion.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EncoderRNN(\n",
      "  (embedding): Embedding(5747, 256)\n",
      "  (gru): GRU(256, 256, batch_first=True)\n",
      ")\n",
      "DecoderRNN(\n",
      "  (embedding): Embedding(5634, 256)\n",
      "  (gru): GRU(256, 256, batch_first=True)\n",
      "  (out): Linear(in_features=256, out_features=5634, bias=True)\n",
      "  (sm): LogSoftmax()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(encoder)\n",
    "print(decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_opt = RMSprop(encoder.parameters(), lr=LR)\n",
    "de_opt = RMSprop(decoder.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_action(*args,**kwargs):\n",
    "    s1,s2 = args[0]\n",
    "    if CUDA:\n",
    "        s1,s2 = s1.cuda(),s2.cuda()\n",
    "        \n",
    "    batch_size = s1.size()[0]\n",
    "    target_length = s2.size()[1]\n",
    "    \n",
    "    en_opt.zero_grad()\n",
    "    de_opt.zero_grad()\n",
    "    \n",
    "    encoder_hidden = encoder.initHidden(batch_size)\n",
    "    encoder_output, encoder_hidden = encoder(s1,encoder_hidden)\n",
    "    \n",
    "    decoder_hidden = encoder_hidden # encoder passing hidden state to decoder!\n",
    "    \n",
    "    decoder_input  = decoder.initInput(batch_size)\n",
    "    \n",
    "    loss = 0\n",
    "    for seq_idx in range(target_length):\n",
    "        decoder_output, decoder_hidden = decoder(decoder_input,decoder_hidden)\n",
    "        \n",
    "        idx_target = s2[:,seq_idx]\n",
    "        \n",
    "        loss += criterion(decoder_output,idx_target)\n",
    "        decoder_input = idx_target # teacher forcing\n",
    "        \n",
    "    loss.backward()\n",
    "    \n",
    "    en_opt.step()\n",
    "    de_opt.step()\n",
    "    return {\n",
    "        \"loss\":loss.item(),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(dataset=ds,batch_size=4,print_on=2)\n",
    "trainer.train_data.collate_fn = pad_collate\n",
    "trainer.action = train_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainer.train(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
